{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install lm-eval==0.4.9.1\n!pip install autoawq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport lm_eval\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom datasets import load_dataset\nfrom peft import PeftModel\nimport torch\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# mcq","metadata":{}},{"cell_type":"code","source":"mcq_task = \"\"\"\ntask: mcq\ndataset_path: vohuutridung/Public-Test\ndataset_name: mcq\noutput_type: multiple_choice\nvalidation_split: train\ndoc_to_text: \"Câu hỏi: {{question}}\\n\\nCác lựa chọn:\\n{% for c in choices %}{{ loop.index0 }}. {{ c }}\\n{% endfor %}\\n\\nĐáp án đúng là:\"\ndoc_to_target: answer\ndoc_to_choice: choices\nmetric_list:\n  - metric: acc\n\n\"\"\"\nwith open(\"mcq.yaml\", \"w\") as f:\n    f.write(mcq_task)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%env LOGLEVEL=INFO\n!lm_eval \\\n    --model hf \\\n    --model_args pretrained=vohuutridung/qwen3-1.7b-legal-pretrain,peft=vohuutridung/merged3-v8 \\\n    --include_path ./ \\\n    --tasks mcq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# nli","metadata":{}},{"cell_type":"code","source":"nli_task = \"\"\"\ntask: nli\ndataset_path: vohuutridung/Public-Test\ndataset_name: nli\noutput_type: multiple_choice\nvalidation_split: train\ndoc_to_text: \"Đoạn luật sau đây: {{legal_document}}\\n\\nCâu hỏi chi tiết: {{specific_question}}\\n\\nCâu hỏi: {{question}}\\n\\nĐáp án đúng là:\"\ndoc_to_target: answer\ndoc_to_choice: choices\nmetric_list:\n  - metric: acc\n\"\"\"\nwith open('nli.yaml', 'w') as f:\n    f.write(nli_task)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%env LOGLEVEL=INFO\n!lm_eval \\\n    --model hf \\\n    --model_args pretrained=vohuutridung/qwen3-1.7b-legal-pretrain,peft=vohuutridung/merged3-v8 \\\n    --include_path ./ \\\n    --tasks nli","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# sqa","metadata":{}},{"cell_type":"code","source":"TEMPLATE = \"\"\"\n    You are a neutral and highly reliable legal judge AI.\n    Your responsibility is to evaluate a model-generated legal reasoning based on Vietnamese law using strict, unbiased, reference-grounded criteria.\n    \n    IMPORTANT RULES:\n    - You MUST NOT hallucinate legal rules not present in the question or the ground truth.\n    - You MUST NOT evaluate based on style, wording, or length.\n    - Your evaluation must be based ONLY on legal correctness, logical reasoning, and factual alignment with the ground truth.\n    - You must avoid position bias: do NOT favor the ground truth blindly; evaluate logically.\n    - You must avoid verbosity bias: short or long answers are NOT penalized.\n    - You must avoid semantic drift: stay anchored to the question and ground truth.\n\n    You will be given:\n    1. The legal question\n    2. The ground-truth expert answer\n    3. The model-generated answer\n\n    ---QUESTION---\n    {question}\n    ---END QUESTION---\n\n    --- GROUND TRUTH ANSWER ---\n    {answer}\n    --- END GROUND TRUTH ANSWER ---\n\n    --- MODEL ANSWER ---\n    {response}\n    --- END MODEL ANSWER ---\n\n    Your evaluation criteria (equal weight):\n    1. **Major Premise Accuracy** – correct identification of relevant legal norms  \n    2. **Minor Premise Accuracy** – correct interpretation of facts  \n    3. **Logical Structure (Syllogism)** – correct linkage between premises and conclusion  \n    4. **Legal Compliance** – aligns with Vietnamese law  \n    5. **Faithfulness** – no contradictions with ground truth reasoning  \n    6. **Absence of Hallucination** – does not invent legal facts, rules, or conclusions  \n    7. **Completeness** – fully addresses the question  \n    8. **Clarity** – reasoning is understandable and coherent \n\n    Provide a **score from 1 to 10**.\n    Your response MUST be ONLY the number.\n    \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------EVAL---------------------------------------------------\ndef prepare_eval_model():\n    eval_model_name = 'vohuutridung/qwen3-1.7b-legal-pretrain'\n    eval_adapter = 'vohuutridung/merged3-v8' #*************************************************************************\n    \n    eval_tokenizer = AutoTokenizer.from_pretrained(eval_model_name)\n    eval_model = AutoModelForCausalLM.from_pretrained(\n        eval_model_name,\n        torch_dtype=\"auto\",\n        device_map=\"balanced\"\n    )\n    if eval_adapter:\n        eval_model = PeftModel.from_pretrained(eval_model, eval_adapter)\n        eval_model = eval_model.merge_and_unload()\n        print('Merged lora adapter to base model for faster inference.')\n\n    eval_model.eval()\n    return eval_model, eval_tokenizer\n\n\ndef prepare_eval_dataset():\n    dataset = load_dataset('vohuutridung/Public-Test', 'sqa', split='train')\n    print(dataset)\n    return dataset\n\n\ndef generate_response(model, tokenizer, question):\n    prompt = (\n        \"Bạn là một chuyên gia pháp luật Việt Nam.\\n\"\n        \"Nhiệm vụ của bạn là phân tích tình huống và áp dụng quy định pháp luật hiện hành \"\n        \"để suy luận theo kiểu suy luận logic (syllogism) và đưa ra kết luận rõ ràng.\\n\\n\"\n        \"YÊU CẦU:\\n\"\n        \"- Trình bày lập luận theo từng ý rõ ràng (có thể chia thành: Tiền đề lớn, Tiền đề nhỏ, Kết luận).\\n\"\n        \"- Dẫn chiếu điều luật, nghị định, văn bản pháp luật liên quan nếu có.\\n\"\n        \"- Cuối cùng nêu KẾT LUẬN rõ ràng về quyền, nghĩa vụ hoặc kết quả pháp lý trong tình huống.\\n\\n\"\n        f\"TÌNH HUỐNG / CÂU HỎI:\\n{question}\\n\\n\"\n        \"TRẢ LỜI:\\n\"\n    )\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=1024,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    return response\n\n# --------------------------------------------JUDGE---------------------------------------------------\ndef prepare_judge_model():\n    judge_model_name = \"Qwen/Qwen3-32B-AWQ\"\n    \n    judge_tokenizer = AutoTokenizer.from_pretrained(judge_model_name)\n    judge_model = AutoModelForCausalLM.from_pretrained(\n        judge_model_name,\n        torch_dtype=\"auto\",\n        device_map='balanced',\n    )\n    judge_model.eval()\n\n    return judge_model, judge_tokenizer\n\n\ndef judge(model, tokenizer, q, a, r):\n    prompt = TEMPLATE.format(question=q, answer=a, response=r)\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n    \n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=16,\n        do_sample=False\n    )\n    output = tokenizer.decode(\n        generated_ids[0][model_inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_model, eval_tokenizer = prepare_eval_model()\ndataset = prepare_eval_dataset()\n\neval_dataset = []\nfor item in tqdm(dataset):\n    question = item['question']\n    answer = item['answer']\n    response = generate_response(eval_model, eval_tokenizer, question)\n    \n    eval_dataset.append({\n        'question': question,\n        'answer': answer,\n        'response': response,\n    })\n    \nprint(f'There are {len(eval_dataset)} eval samples.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"judge_model, judge_tokenizer = prepare_judge_model()\n\nscores = []\nfor sample in tqdm(eval_dataset):\n    q, a, r = sample['question'], sample['answer'], sample['response']\n    prompt = TEMPLATE.format(question=q, answer=a, response=r)\n    output = judge(judge_model, judge_tokenizer, q, a, r)\n    scores.append(int(output))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"round((sum(scores) / len(scores)) * 10, 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}