{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:29:57.911329Z",
     "iopub.status.busy": "2025-11-25T08:29:57.911146Z",
     "iopub.status.idle": "2025-11-25T08:29:59.473604Z",
     "shell.execute_reply": "2025-11-25T08:29:59.472770Z",
     "shell.execute_reply.started": "2025-11-25T08:29:57.911305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:29:59.475765Z",
     "iopub.status.busy": "2025-11-25T08:29:59.475308Z",
     "iopub.status.idle": "2025-11-25T08:29:59.479690Z",
     "shell.execute_reply": "2025-11-25T08:29:59.478962Z",
     "shell.execute_reply.started": "2025-11-25T08:29:59.475744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:29:59.481062Z",
     "iopub.status.busy": "2025-11-25T08:29:59.480494Z",
     "iopub.status.idle": "2025-11-25T08:32:06.382523Z",
     "shell.execute_reply": "2025-11-25T08:32:06.381516Z",
     "shell.execute_reply.started": "2025-11-25T08:29:59.481024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bitsandbytes\n",
    "!pip uninstall -y transformers tokenizers huggingface_hub\n",
    "!pip install transformers -U #==4.45.2\n",
    "!pip install tokenizers==0.21.0\n",
    "!pip install huggingface_hub==0.24.6\n",
    "!pip install trl==0.9.6 peft==0.11.1 accelerate==1.0.0\n",
    "!pip uninstall -y pyarrow\n",
    "!pip install pyarrow==14.0.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:06.383978Z",
     "iopub.status.busy": "2025-11-25T08:32:06.383749Z",
     "iopub.status.idle": "2025-11-25T08:32:10.103354Z",
     "shell.execute_reply": "2025-11-25T08:32:10.102501Z",
     "shell.execute_reply.started": "2025-11-25T08:32:06.383954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patch_accelerator():\n",
    "    try:\n",
    "        from accelerate import Accelerator\n",
    "        original_unwrap = Accelerator.unwrap_model\n",
    "        \n",
    "        def patched_unwrap(self, model, keep_fp32_wrapper=True, keep_torch_compile=False):\n",
    "            try:\n",
    "                return original_unwrap(self, model, keep_fp32_wrapper=keep_fp32_wrapper, keep_torch_compile=keep_torch_compile)\n",
    "            except TypeError:\n",
    "                return original_unwrap(self, model, keep_fp32_wrapper=keep_fp32_wrapper)\n",
    "        \n",
    "        Accelerator.unwrap_model = patched_unwrap\n",
    "        print(\"âœ“ Accelerator patched successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not patch Accelerator: {e}\")\n",
    "\n",
    "patch_accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:10.104566Z",
     "iopub.status.busy": "2025-11-25T08:32:10.104172Z",
     "iopub.status.idle": "2025-11-25T08:32:36.760943Z",
     "shell.execute_reply": "2025-11-25T08:32:36.760307Z",
     "shell.execute_reply.started": "2025-11-25T08:32:10.104525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:36.762244Z",
     "iopub.status.busy": "2025-11-25T08:32:36.761690Z",
     "iopub.status.idle": "2025-11-25T08:32:36.766736Z",
     "shell.execute_reply": "2025-11-25T08:32:36.765869Z",
     "shell.execute_reply.started": "2025-11-25T08:32:36.762212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:36.769364Z",
     "iopub.status.busy": "2025-11-25T08:32:36.769171Z",
     "iopub.status.idle": "2025-11-25T08:32:37.002248Z",
     "shell.execute_reply": "2025-11-25T08:32:37.001598Z",
     "shell.execute_reply.started": "2025-11-25T08:32:36.769348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskConfig:\n",
    "    task_name: str\n",
    "    dataset_path: str = 'vohuutridung/Vietnamese-Legal-Chat-Dataset'\n",
    "    dataset_split: str = 'train'\n",
    "    model_name: str = 'vohuutridung/qwen3-1.7b-legal-pretrain'\n",
    "    dtype: torch.dtype = torch.bfloat16\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    target_modules: list[str] | str = \"all-linear\"\n",
    "    train_batch_size: int = 2\n",
    "    eval_batch_size: int = 8\n",
    "    epochs: int = 3\n",
    "    logging_steps: int = 50\n",
    "    save_total_limit: int = 2\n",
    "    lr: float = 2e-4\n",
    "    eval_steps: int = 50\n",
    "    eval_strategy: str = \"steps\"\n",
    "    max_seq_length: int = 4096\n",
    "    push_to_hub: bool = False\n",
    "    hf_repo_name: str = 'vohuutridung/stage1-mcq' # remember to change\n",
    "    \n",
    "\n",
    "class Stage1Trainer:\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        self.config = config\n",
    "        logger.info(f'Initializing trainer for task: {config.task_name}')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            dtype=config.dtype,\n",
    "        )\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model.config.use_cache = False\n",
    "        logger.info('Model & Tokenizer loaded successfully.')\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            target_modules=config.target_modules,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        # self.model.enable_input_require_grads()\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        texts = []\n",
    "        conversations = examples[\"conversations\"]\n",
    "        \n",
    "        for conv in conversations:\n",
    "            try:\n",
    "                if not conv or len(conv) < 2:\n",
    "                    logger.warning(f\"Skipping empty or incomplete conversation: {conv}\")\n",
    "                    continue\n",
    "                \n",
    "                messages = []\n",
    "                for msg in conv:\n",
    "                    role = msg.get('from', '')\n",
    "                    if role == 'human':\n",
    "                        role = 'user'\n",
    "                    elif role == 'gpt':\n",
    "                        role = 'assistant'\n",
    "                    else:\n",
    "                        role = 'user' if len(messages) % 2 == 0 else 'assistant'\n",
    "                    \n",
    "                    content = msg.get('value', '')\n",
    "                    if content:  \n",
    "                        messages.append({\n",
    "                            \"role\": role,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "                \n",
    "                if len(messages) >= 2: \n",
    "                    text = self.tokenizer.apply_chat_template(\n",
    "                        messages, \n",
    "                        tokenize=False, \n",
    "                        add_generation_prompt=False\n",
    "                    )\n",
    "                    texts.append(text)\n",
    "                else:\n",
    "                    logger.warning(f\"Not enough valid messages in conversation: {messages}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process conversation: {e}\")\n",
    "                logger.error(f\"Conversation: {conv}\")\n",
    "                continue\n",
    "        \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        total_dataset = load_dataset(self.config.dataset_path, split=self.config.dataset_split)\n",
    "        \n",
    "        if self.config.task_name == 'mcq': dataset = total_dataset.select(range(803))\n",
    "        elif self.config.task_name == 'nli': dataset = total_dataset.select(range(803, 803+745))\n",
    "        elif self.config.task_name == 'sqa': dataset = total_dataset.select(range(803+745, len(total_dataset)))\n",
    "        logger.info('Dataset loaded successfully.')\n",
    "        logger.info(dataset)\n",
    "        \n",
    "        num_train = int(0.95 * len(dataset))\n",
    "        train_dataset = dataset.select(range(num_train))\n",
    "        eval_dataset = dataset.select(range(num_train, len(dataset)))\n",
    "        logger.info(f'{len(train_dataset)} train samples.')\n",
    "        logger.info(f'{len(eval_dataset)} eval samples.')\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        logger.info('=' * 50)\n",
    "        logger.info(f'Starting training for task: {self.config.task_name}')\n",
    "        logger.info('=' * 50)\n",
    "        \n",
    "        train_dataset, eval_dataset = self.prepare_dataset()\n",
    "        \n",
    "        args = SFTConfig(\n",
    "            output_dir=f'./{self.config.task_name}',\n",
    "            \n",
    "            per_device_train_batch_size=self.config.train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.eval_batch_size,\n",
    "            num_train_epochs=self.config.epochs,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            \n",
    "            learning_rate=self.config.lr,\n",
    "            max_seq_length=self.config.max_seq_length,\n",
    "\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=[\"tensorboard\"],\n",
    "\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_8bit\",\n",
    "            group_by_length=True,\n",
    "        )\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            formatting_func=self.preprocess_function,\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logger.info('Start training.')\n",
    "        trainer.train()\n",
    "        logger.info('Training finished.')\n",
    "        \n",
    "        adapter_dir = f'./{self.config.task_name}/lora'\n",
    "        trainer.model.save_pretrained(adapter_dir)\n",
    "        trainer.processing_class.save_pretrained(adapter_dir)\n",
    "        logger.info(f\"Saved LoRA adapter for task {self.config.task_name} at {adapter_dir}\") \n",
    "\n",
    "        if self.config.push_to_hub:\n",
    "            user_secrets = UserSecretsClient()\n",
    "            HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "            login(HF_TOKEN)\n",
    "            \n",
    "            trainer.model.push_to_hub(self.config.hf_repo_name)\n",
    "            trainer.processing_class.push_to_hub(self.config.hf_repo_name)\n",
    "            logger.info(f'Pushed LoRA adapter for task {self.config.task_name} to {self.config.hf_repo_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:37.003192Z",
     "iopub.status.busy": "2025-11-25T08:32:37.002971Z",
     "iopub.status.idle": "2025-11-25T08:32:37.019291Z",
     "shell.execute_reply": "2025-11-25T08:32:37.018562Z",
     "shell.execute_reply.started": "2025-11-25T08:32:37.003175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mcq_config = TaskConfig(\n",
    "#     task_name='mcq',\n",
    "#     epochs=3,\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_name='vohuutridung/stage1-mcq-v3-3e',\n",
    "# )\n",
    "\n",
    "# mcq_trainer = Stage1Trainer(mcq_config)\n",
    "# mcq_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T08:32:37.020508Z",
     "iopub.status.busy": "2025-11-25T08:32:37.020128Z",
     "iopub.status.idle": "2025-11-25T08:32:37.033691Z",
     "shell.execute_reply": "2025-11-25T08:32:37.033018Z",
     "shell.execute_reply.started": "2025-11-25T08:32:37.020482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# nli_config = TaskConfig(\n",
    "#     task_name='nli',\n",
    "#     epochs=2,\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_name='vohuutridung/stage1-nli-v2-2e',\n",
    "# )\n",
    "\n",
    "# nli_trainer = Stage1Trainer(nli_config)\n",
    "# nli_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-25T09:14:43.782Z",
     "iopub.execute_input": "2025-11-25T08:32:37.034739Z",
     "iopub.status.busy": "2025-11-25T08:32:37.034461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sqa_config = TaskConfig(\n",
    "    task_name='sqa',\n",
    "    train_batch_size=4,\n",
    "    eval_batch_size=4,\n",
    "    epochs=5,\n",
    "    push_to_hub=True,\n",
    "    hf_repo_name='vohuutridung/stage1-sqa-v2-5e',\n",
    ")\n",
    "\n",
    "sqa_trainer = Stage1Trainer(sqa_config)\n",
    "sqa_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
