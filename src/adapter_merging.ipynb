{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:32:21.314033Z","iopub.execute_input":"2025-12-23T04:32:21.314389Z","iopub.status.idle":"2025-12-23T04:32:21.668695Z","shell.execute_reply.started":"2025-12-23T04:32:21.314364Z","shell.execute_reply":"2025-12-23T04:32:21.667582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel, get_peft_model_state_dict\nfrom transformers import AutoModelForCausalLM\nimport torch\nfrom huggingface_hub import login, HfApi, create_repo\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:32:21.670070Z","iopub.execute_input":"2025-12-23T04:32:21.670524Z","iopub.status.idle":"2025-12-23T04:33:06.187315Z","shell.execute_reply.started":"2025-12-23T04:32:21.670492Z","shell.execute_reply":"2025-12-23T04:33:06.186234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model_name = 'vohuutridung/qwen3-1.7b-legal-pretrain'\nmcq_adapter = 'vohuutridung/stage1-mcq'\nnli_adapter = 'vohuutridung/stage1-nli-v1-3e'\nsqa_adapter = 'vohuutridung/stage1-sqa-v1-3e'\n\nrepo_id = \"vohuutridung/merged3-v9\" # repo to push the merged adapter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:33:06.188276Z","iopub.execute_input":"2025-12-23T04:33:06.189043Z","iopub.status.idle":"2025-12-23T04:33:06.194282Z","shell.execute_reply.started":"2025-12-23T04:33:06.189008Z","shell.execute_reply":"2025-12-23T04:33:06.193163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(base_model, mcq_adapter, adapter_name='mcq')\n_ = model.load_adapter(nli_adapter, adapter_name='nli')\n_ = model.load_adapter(sqa_adapter, adapter_name='sqa')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:33:06.196681Z","iopub.execute_input":"2025-12-23T04:33:06.197091Z","iopub.status.idle":"2025-12-23T04:33:50.770423Z","shell.execute_reply.started":"2025-12-23T04:33:06.197066Z","shell.execute_reply":"2025-12-23T04:33:50.768004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"adapters = ['mcq', 'nli','sqa']\nweights = [1.0, 1.0, 1.5]\nnew_adapter_name = 'merge'\ncombination_type = 'dare_ties_svd'\nmajority_sign_method = 'frequency'\ndensity = 0.8\n\nmodel.add_weighted_adapter(\n    adapters,\n    weights,\n    new_adapter_name,\n    combination_type=combination_type,\n    majority_sign_method=majority_sign_method,\n    density=density,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:33:50.774308Z","iopub.execute_input":"2025-12-23T04:33:50.774955Z","iopub.status.idle":"2025-12-23T04:49:31.433749Z","shell.execute_reply.started":"2025-12-23T04:33:50.774906Z","shell.execute_reply":"2025-12-23T04:49:31.432643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.set_adapter(new_adapter_name)\nmodel.delete_adapter('mcq')\nmodel.delete_adapter('nli')\nmodel.delete_adapter('sqa')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:31.435415Z","iopub.execute_input":"2025-12-23T04:49:31.435811Z","iopub.status.idle":"2025-12-23T04:49:31.581092Z","shell.execute_reply.started":"2025-12-23T04:49:31.435782Z","shell.execute_reply":"2025-12-23T04:49:31.580094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if \"lora_\" in name:\n        param.data = param.data.contiguous()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:31.582330Z","iopub.execute_input":"2025-12-23T04:49:31.582703Z","iopub.status.idle":"2025-12-23T04:49:32.167828Z","shell.execute_reply.started":"2025-12-23T04:49:31.582670Z","shell.execute_reply":"2025-12-23T04:49:32.166557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('merge_adapter')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:32.169497Z","iopub.execute_input":"2025-12-23T04:49:32.169880Z","iopub.status.idle":"2025-12-23T04:49:34.057173Z","shell.execute_reply.started":"2025-12-23T04:49:32.169847Z","shell.execute_reply":"2025-12-23T04:49:34.056034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(HF_TOKEN)\napi = HfApi()\n\nlocal_dir = f\"merge_adapter/{new_adapter_name}\"\ncreate_repo(repo_id, exist_ok=True)\n\nfor filename in os.listdir(local_dir):\n    local_path = os.path.join(local_dir, filename)\n\n    if os.path.isfile(local_path):\n        print(f\"Uploading {filename} ...\")\n        api.upload_file(\n            path_or_fileobj=local_path,\n            path_in_repo=filename,\n            repo_id=repo_id,\n        )\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:34.058504Z","iopub.execute_input":"2025-12-23T04:49:34.058840Z","iopub.status.idle":"2025-12-23T04:49:41.721673Z","shell.execute_reply.started":"2025-12-23T04:49:34.058801Z","shell.execute_reply":"2025-12-23T04:49:41.720689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n\nadapter_path = repo_id\nmodel = PeftModel.from_pretrained(\n    base_model,\n    adapter_path,\n    is_trainable=True,\n)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:41.724562Z","iopub.execute_input":"2025-12-23T04:49:41.724948Z","iopub.status.idle":"2025-12-23T04:49:48.337676Z","shell.execute_reply.started":"2025-12-23T04:49:41.724923Z","shell.execute_reply":"2025-12-23T04:49:48.336659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sd = get_peft_model_state_dict(model)\nprint(len(sd))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:49:48.338895Z","iopub.execute_input":"2025-12-23T04:49:48.339409Z","iopub.status.idle":"2025-12-23T04:49:48.667907Z","shell.execute_reply.started":"2025-12-23T04:49:48.339377Z","shell.execute_reply":"2025-12-23T04:49:48.666635Z"}},"outputs":[],"execution_count":null}]}