{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Script for training variants for comparison:\n",
    "      - Full Fine-tuning\n",
    "      - LoRA Training Only\n",
    "      - QLoRA Training Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!{sys.executable} -m pip install bitsandbytes\n",
    "!{sys.executable} -m pip install trl==0.9.6 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install bitsandbytes\n",
    "# !pip uninstall -y transformers tokenizers huggingface_hub\n",
    "# !pip install transformers -U #==4.45.2\n",
    "# !pip install tokenizers==0.21.0\n",
    "# !pip install huggingface_hub==0.24.6\n",
    "# !pip install trl==0.9.6 peft==0.11.1 accelerate==1.0.0\n",
    "# !pip uninstall -y pyarrow\n",
    "# !pip install pyarrow==14.0.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patch_accelerator():\n",
    "#     try:\n",
    "#         from accelerate import Accelerator\n",
    "#         original_unwrap = Accelerator.unwrap_model\n",
    "        \n",
    "#         def patched_unwrap(self, model, keep_fp32_wrapper=True, keep_torch_compile=False):\n",
    "#             try:\n",
    "#                 return original_unwrap(self, model, keep_fp32_wrapper=keep_fp32_wrapper, keep_torch_compile=keep_torch_compile)\n",
    "#             except TypeError:\n",
    "#                 return original_unwrap(self, model, keep_fp32_wrapper=keep_fp32_wrapper)\n",
    "        \n",
    "#         Accelerator.unwrap_model = patched_unwrap\n",
    "#         print(\"âœ“ Accelerator patched successfully\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: Could not patch Accelerator: {e}\")\n",
    "\n",
    "# patch_accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskConfig:\n",
    "    full: bool = False\n",
    "    lora: bool = False\n",
    "    qlora: bool = False\n",
    "    \n",
    "    dataset_path: str = 'vohuutridung/Vietnamese-Legal-Chat-Dataset'\n",
    "    dataset_split: str = 'train'\n",
    "    model_name: str = 'vohuutridung/qwen3-1.7b-legal-pretrain'\n",
    "    \n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dtype: torch.dtype = torch.bfloat16\n",
    "    \n",
    "    train_batch_size: int = 2\n",
    "    eval_batch_size: int = 8\n",
    "    epochs: int = 3\n",
    "    logging_steps: int = 50\n",
    "    save_total_limit: int = 2\n",
    "    lr: float = 5e-5\n",
    "    eval_steps: int = 50\n",
    "    eval_strategy: str = \"steps\"\n",
    "    max_seq_length: int = 4096\n",
    "    \n",
    "    push_to_hub: bool = False\n",
    "    hf_repo_name: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.lr = 2e-4 if not self.full else 5e-5\n",
    "        \n",
    "        if self.push_to_hub and not self.hf_repo_name:\n",
    "            raise ValueError(\"hf_repo_name is required when push_to_hub=True\")\n",
    "\n",
    "        if self.lora or self.qlora:\n",
    "            self.lora_config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules='all-linear',\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "            \n",
    "        if self.qlora:\n",
    "            self.qlora_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "\n",
    "    \n",
    "class Stage2Trainer:\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        logger.info(f\"Model loaded in mode: \"\n",
    "                    f\"{'FULL' if config.full else 'LoRA' if config.lora else 'QLoRA'}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.config.full:\n",
    "            return AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                dtype=self.config.dtype,\n",
    "            )\n",
    "            \n",
    "        if self.config.lora:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                dtype=self.config.dtype,\n",
    "            )\n",
    "            model = get_peft_model(model, self.config.lora_config)\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "            return model\n",
    "\n",
    "        if self.config.qlora:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                quantization_config=self.config.qlora_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "            model = get_peft_model(model, self.config.lora_config)\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "            return model\n",
    "\n",
    "    \n",
    "    def preprocess_function(self, examples):\n",
    "        texts = []\n",
    "        conversations = examples[\"conversations\"]\n",
    "        \n",
    "        for conv in conversations:\n",
    "            try:\n",
    "                if not conv or len(conv) < 2:\n",
    "                    logger.warning(f\"Skipping empty or incomplete conversation: {conv}\")\n",
    "                    continue\n",
    "                \n",
    "                messages = []\n",
    "                for msg in conv:\n",
    "                    role = msg.get('from', '')\n",
    "                    if role == 'human':\n",
    "                        role = 'user'\n",
    "                    elif role == 'gpt':\n",
    "                        role = 'assistant'\n",
    "                    else:\n",
    "                        role = 'user' if len(messages) % 2 == 0 else 'assistant'\n",
    "                    \n",
    "                    content = msg.get('value', '')\n",
    "                    if content:  \n",
    "                        messages.append({\n",
    "                            \"role\": role,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "                \n",
    "                if len(messages) >= 2: \n",
    "                    text = self.tokenizer.apply_chat_template(\n",
    "                        messages, \n",
    "                        tokenize=False, \n",
    "                        add_generation_prompt=False\n",
    "                    )\n",
    "                    texts.append(text)\n",
    "                else:\n",
    "                    logger.warning(f\"Not enough valid messages in conversation: {messages}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process conversation: {e}\")\n",
    "                logger.error(f\"Conversation: {conv}\")\n",
    "                continue\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        dataset = load_dataset(self.config.dataset_path, split=self.config.dataset_split)\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "        logger.info('Dataset loaded and shuffled successfully.')\n",
    "        logger.info(dataset)\n",
    "\n",
    "        num_train = int(0.95 * len(dataset))\n",
    "        train_dataset = dataset.select(range(num_train))\n",
    "        eval_dataset = dataset.select(range(num_train, len(dataset)))\n",
    "        logger.info(f'{len(train_dataset)} train samples.')\n",
    "        logger.info(f'{len(eval_dataset)} eval samples.')\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        logger.info('=' * 50)\n",
    "        logger.info(f'Stepping into training process')\n",
    "        logger.info('=' * 50)\n",
    "        \n",
    "        train_dataset, eval_dataset = self.prepare_dataset()\n",
    "        \n",
    "        args = SFTConfig(\n",
    "            output_dir='./output',\n",
    "            \n",
    "            per_device_train_batch_size=self.config.train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.eval_batch_size,\n",
    "            num_train_epochs=self.config.epochs,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            \n",
    "            learning_rate=self.config.lr,\n",
    "\n",
    "            max_seq_length=self.config.max_seq_length,\n",
    "\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,\n",
    "        )\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            formatting_func=self.preprocess_function,\n",
    "        )\n",
    "\n",
    "        logger.info('Start training.')\n",
    "        trainer.train()\n",
    "        logger.info('Training finished.')\n",
    "\n",
    "        if self.config.push_to_hub:\n",
    "            # user_secrets = UserSecretsClient()\n",
    "            # HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "            HF_TOKEN = 'HF_TOKEN'\n",
    "            login(HF_TOKEN)\n",
    "            \n",
    "            trainer.model.push_to_hub(self.config.hf_repo_name)\n",
    "            trainer.processing_class.push_to_hub(self.config.hf_repo_name)\n",
    "            logger.info(f'Pushed to {self.config.hf_repo_name} successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_config = TaskConfig(\n",
    "#     full=True,\n",
    "#     train_batch_size=1,\n",
    "#     eval_batch_size=4,\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_name='vohuutridung/3150-fullft-v2',\n",
    "# )\n",
    "# full_trainer = Stage2Trainer(full_config)\n",
    "# full_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f540cc97b56940c4b8677fd4dd5f2dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908ad269619f4cff8450b1515aaca2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbcc065d43a471e8d77200d5c6aee96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b313ee1c50d649c78336f5a2a0a8a7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f077fd5e6ded4f5bb49dc423f08c2c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c4f98e2e5e411dbc82d8d3523b580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a9b18d983049f89155183c73facfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28207c0ef1343fbbef731270e53759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84f4bf177a44b7c8ddace261f6d82b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daae0489d0f442fa825f377f8bd991c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33089498bb9343169fdcda9538f819f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcc788c42f04de891245ffcff1c2419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca30a79f6d34529832da9c38a25a27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c6b294315c4430838ed8622f03d6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\n",
      "INFO:__main__:Model loaded in mode: LoRA\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:Stepping into training process\n",
      "INFO:__main__:==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e27853660c422792f714569afe55e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64de11949cc240a7a11b7f79dffba492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52ed2779ca14c9fad07525fb2aaff87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3537 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded and shuffled successfully.\n",
      "INFO:__main__:Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 3537\n",
      "})\n",
      "INFO:__main__:3360 train samples.\n",
      "INFO:__main__:177 eval samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function SFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize at 0x7f321390f7f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function SFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize at 0x7f321390f7f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b0e951fbdd4880a2fd7fbda79b72f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417e0eeaf82140fa857c4bbd33db565b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10080' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10080/10080 1:47:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.185400</td>\n",
       "      <td>1.032971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.026000</td>\n",
       "      <td>0.988788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>0.969743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.955773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.967400</td>\n",
       "      <td>0.950011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.947552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>0.939271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.930669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>0.934949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.922995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.948300</td>\n",
       "      <td>0.919456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.919246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.907700</td>\n",
       "      <td>0.915010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.888800</td>\n",
       "      <td>0.911569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.910608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>0.904525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>0.904337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.901499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.896960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.896387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.894257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.850600</td>\n",
       "      <td>0.891264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.889673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.880358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>0.880054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.873229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.814300</td>\n",
       "      <td>0.875491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.875966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.880670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.873460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.890200</td>\n",
       "      <td>0.871041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.848800</td>\n",
       "      <td>0.871565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.867453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.866797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.864375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>0.866122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.872900</td>\n",
       "      <td>0.860058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.863719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.856859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.857238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.852694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>0.851691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>0.850785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.849768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.846288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.846493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.845309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.848197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>0.845036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.844384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.840698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.842823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.909300</td>\n",
       "      <td>0.837369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.837048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.836906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>0.834349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.835500</td>\n",
       "      <td>0.834241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.833517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.830559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.834252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.868700</td>\n",
       "      <td>0.828241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>0.833668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>0.830199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.826630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.825271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.838088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.830135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.829825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.830367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.832526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.723900</td>\n",
       "      <td>0.827253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.832768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>0.831645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.829026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.825370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.828236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.826573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.832228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.827089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.830708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>0.827701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.823798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.701800</td>\n",
       "      <td>0.824544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.720800</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.829307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.781100</td>\n",
       "      <td>0.823714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>0.823225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.822173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.821110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.821568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.824495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>0.819973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.818690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.821088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>0.819148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.818807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.817912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.725800</td>\n",
       "      <td>0.813803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.818048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.815464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.814302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.745200</td>\n",
       "      <td>0.814019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.814611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.811874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.811476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.812331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.812156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.810659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.810044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.809553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.809513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.808214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.808794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.806714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.803548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.801360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.805540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.802141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.802437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.663700</td>\n",
       "      <td>0.803148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.801452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.658200</td>\n",
       "      <td>0.801444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.800778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.798304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.798232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.797807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>0.796811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.796996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.797266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>0.798636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.586100</td>\n",
       "      <td>0.819098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.513700</td>\n",
       "      <td>0.827147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.832906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.829758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.825631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.829093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.835496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.831148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.828627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.830362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.831224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.829804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.520600</td>\n",
       "      <td>0.830612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.831344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.834595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.836333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.832398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.832325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>0.834090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.830364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.830415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.833724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.831467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.830399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.827537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.826370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.824135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.828410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.828621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>0.830175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.828629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.828786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.828873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.824320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.825987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.828051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.537800</td>\n",
       "      <td>0.827876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.827252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.825708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.829309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.828826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.825556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.826567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.825152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>0.824976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.825166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.825182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.827021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.825975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.823788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.822414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.823429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.823301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.823996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.823755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.824813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.824436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.519400</td>\n",
       "      <td>0.824080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.824818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.824025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.822739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.530200</td>\n",
       "      <td>0.823307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.823764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.823696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.824058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.824423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.824140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training finished.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaa2ee7f3ae46cab3a4dfae2ae7a19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81adbdb09b6147e989e93e5a725d2411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c4fb1fc9c44f089d89892d9e707c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...pnl8xa0z5/adapter_model.safetensors:   0%|          | 45.8kB / 69.8MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cb12c85744496ea9efed5d10973574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3813f2f400954318ac37224ed84e76ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ac8d2a675d4d598899632d5eb060cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21552d035bb24d6aaf5bf7844e561d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp71v0farm/tokenizer.json       :  97%|#########7| 11.1MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pushed to vohuutridung/3150-lora-v2 successfully.\n"
     ]
    }
   ],
   "source": [
    "train_lora_config = TaskConfig(\n",
    "    lora=True,\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=4,\n",
    "    push_to_hub=True,\n",
    "    hf_repo_name='vohuutridung/3150-lora-v2',\n",
    ")\n",
    "train_lora_trainer = Stage2Trainer(train_lora_config)\n",
    "train_lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_qlora_config = TaskConfig(\n",
    "#     qlora=True,\n",
    "#     train_batch_size=4,\n",
    "#     eval_batch_size=4,\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_name='vohuutridung/3150-qlora',\n",
    "# )\n",
    "# train_qlora_trainer = Stage2Trainer(train_qlora_config)\n",
    "# train_qlora_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
